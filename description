 
1  简介
1.1  背景
物联设备上报的某一时刻的日志。包含设备的唯一标识、上报时间、具体属性及属性对应的数据。
通过规则分析物联设备上报的日志能够生成一些有意义的事件。
1.1.1 物联设备日志之间的关系
目前大多数的物联设备的可靠性不高，导致上报的日志也不可靠。对不可靠的单条日志进行规则判断来生成事件会容易引起误报问题，所以需要考虑连续一段时间内的物联设备日志之间的关系。根据资料可知，如果发生特定事件（火灾），则同一区域的多个物联设备（烟感探测器、温度传感器等）在一段时间内会持续报警。反映到日志上，同一区域的物联设备会持续上报告警信息，即物联设备的日志之间存在某种关系。
1.1.2 传统规则的不足
传统规则（1.2.1小节），大多基于（可靠性不高的）物联设备的一条日志判断某一个属性和阈值之间的关系，或几个属性和阈值之间的关系的逻辑组合来生成（告警）事件。
1.	基于实践，大规模的日志数据（千万级）使用传统数据库（MySQL、Oracle等），导致传统规则执行的时间较长，导致实时性不足。
2.	基于实践，传统规则使用的技术在有限的时间内无法执行复杂的规则（复杂事件处理，Complex Event Processing，CEP）。
3.	基于1和2，传统规则只能执行简单的规则（1.2.1小节）。
4.	针对可靠性不高的物联设备，通过传统规则生成事件不符合逻辑，容易引起误报问题。
1.1.3 规则实现方式
流处理是不断合并新数据以计算结果的动作。在流处理中，输入数据不受限制，并且没有预定的开始或结束。只是形成一系列事件，这些事件到达流处理系统，例如信用卡交易，网站点击或来自物联网设备的传感器读数。
CEP是一种基于事件流的技术，CEP将数据（物联设备日志）看作是不同类型的事件，事件在这里通常是有意义的状态变化，通过分析事件间的关系，利用过滤、关联、聚合等技术，根据事件间的时序关系和聚合关系定制检测规则，持续地从事件流中查询出符合要求的事件序列，最终分析得到更加复杂的复合事件。CEP适合的场景包括实时风险管理、实时交易分析、网络欺诈、网络攻击、市场趋势分享、异常监测等等。
多维算法规则引擎实现步骤：
1.	使用流处理框架(Flink)从消息中间件(Kafka)中读取日志和规则。
2.	对规则进行预处理，对规则使用CEP技术(Siddhi)生成规则引擎。
3.	使用规则引擎处理日志信息，生成事件返回给流处理框架。
4.	流处理框架将生成的事件发送给消息中间件。
多维算法规则引擎相对于传统规则：
1.	采用更加符合物联设备场景的技术框架（流处理框架、消息中间件、CEP）。
2.	没有采用数据库，数据链路更短，处理时间更短。
3.	使用可以避免简单规则带来的误报问题，同时能够实现更加复杂的规则（1.2.2、1.2.3、1.2.4小节）。
1.1.4 实现上相对于Flink CEP的优点
虽然Flink CEP也能够实现复杂的规则，但是在资源消耗上，一个Flink CEP 程序需要启动一个Flink 程序。如果有多个规则，则需要启动多个Flink程序。规则过多很容易把计算资源耗尽。
结合Flink和Siddhi，理论上一个Flink程序能够处理所有的规则，减少了资源消耗，实际上，给处理规则的Flink程序多分配一些资源，就能够流畅的处理所有规则。
1.2  功能
多维算法规则引擎对以下5种规则进行了实现，并保留扩展能力。规则1是其他规则的基础。
1.2.1 规则1
根据同一个产品（类型）多个物联设备上报的日志，筛选出符合下面两个条件中任意一个的设备日志，并生成告警事件。
1.	物联设备的某一个属性和对应阈值之间的关系(e.g:fieldA>0)。
2.	条件1的逻辑组合(e.g:fieldA>0 and filed B<8)。
图1-1 规则1
图1-1中：
1)	不同形状代表不同的产品的设备。
2)	同一形状的不同颜色代表同意产品下的不同设备。
3)	橙色虚线三角形代表符合条件的设备。
1.2.2 规则2
根据一种产品下多个物联设备上报的日志，筛选出依次符合下面4个条件的设备日志，并生成告警事件。
1.	一个设备满足（1.3.1，规则1）的一种实现。
2.	在上一条件的基础上，该设备满足（1.3.1，规则1）的另一种实现。
3.	条件2可以重复多次。
4.	上述3个条件需要在特定时间范围内(e.g:180s)完成。
图1-2中：
1)	不同形状代表不同的产品的设备。
2)	同一形状的不同颜色代表同意产品下的不同设备。
3)	橙色虚线三角的代表符合条件a的设备。
4)	在特定时间范围内，橙色虚线三角形继续满足条件c。


图1-2 规则2
1.2.3 规则3
根据一种产品下多个物联设备上报的日志，筛选出符合下面条件的（一个或多个）设备日志，并生成告警事件。
1.	一个设备A满足（1.3.1，规则1）的一种实现。
2.	在上一条件的基础上，一个设备B满足（1.3.1，规则1）的同一种实现。
3.	条件2可以重复多次。
4.	上述3个条件需要在特定时间范围内(e.g:180s)完成。
图1-3中：
1)	不同形状代表不同的产品的设备。
2)	同一形状的不同颜色代表同意产品下的不同设备。
3)	橙色虚线三角的代表符合条件a的设备。
4)	在特定时间范围内，绿色虚线三角形的代表符合条件a的设备。
5)	在特定时间范围内，蓝色虚线三角形的代表符合条件a的设备。
这三个设备的顺序不能改变。
图1-3 规则3
1.2.4 规则4
根据多种产品下多个设备连续上报的日志，筛选出依次符合下面4个条件的设备日志，并生成告警事件。
1.	一个产品A下的某一个设备a1满足（1.3.1，规则1）的一种实现。
2.	在上一步的基础上，另外一个产品B下的某一个设备b1满足（1.3.1，规则1）的另一种实现。
3.	条件2可以重复多次。
4.	上述3个条件需要在特定时间范围内(e.g:180s)完成。
图1-4中：
1)	不同形状代表不同的产品的设备。
2)	同一形状的不同颜色代表同意产品下的不同设备。
3)	橙色虚线三角的代表符合条件a的设备。
4)	在特定时间范围内，绿色虚线圆型的代表符合条件b的设备。
5)	在特定时间范围内，蓝色虚线五边形的代表符合条件c的设备。
这三个设备的顺序不能改变。



图1-4 规则4
1.2.5 规则5
根据多种产品下多个设备连续上报的日志，筛选出依次符合下面4个条件的设备日志，并生成告警事件。
1.	一个产品A下的某一个设备a1满足（1.3.1，规则1）的一种实现。
2.	在上一步的基础上，一个产品A下的某一个设备a2满足（1.3.1，规则1）的同一种实现。
3.	条件2重复n次。
4.	上述3个条件需要在特定时间范围内(e.g:180s)完成。
图1-5中：
1)	不同形状代表不同的产品的设备。
2)	同一形状的不同颜色代表同意产品下的不同设备。
3)	黄色虚线三角形的代表符合条件a的设备。
4)	在特定时间范围内，绿色虚线三角形的代表符合条件a的设备。
5)	步骤4重复。
6)	在特定时间范围内，黄色虚线三角形的代表符合条件a的设备，同时满足数量为4。
这4个设备的顺序可以改变。

图1-5 规则5
2  架构和流程
2.1  架构
图2-1 规则架构
日志和设备数据从Kafka获取，通过Flink解析日志和规则，将预处理后的数据发送到Siddhi进行规则处理，生成事件。将生成的事件发送到Kafka。
图2-1中，各个节点的含义：
1.	Log：物联设备上报的日志，json格式，存储在Kafka。
2.	Rule：业务系统生成的规则，json格式，存储与Kafka。
3.	Flink：流处理系统，持续接收日志和规则。
4.	ParsedRule：规则引擎从Kafka获取Rule，解析成规则引擎能够识别的规则ParsedRule。
5.	EnabledRule：解析后的规则，规则引擎识别为该规则需要启用，进行规则匹配。
6.	DisabledRule：解析后的规则，规则引擎识别为该规则需要停止，此时停止正在运行的对应的规则。
7.	SiddhiManager：规则系统，用于日志匹配。
8.	SiddhiAppRule：根据EnabledRule生成的规则字符串（类SQL语言）。
9.	SiddhiAppRuntime：根据字符串创建的规则实例和运行时环境。
10.	InputHandle：规则输入系统，负责讲日志发送给规则系统。
11.	RuleResult：根据跟则匹配的结果。
12.	LogMixRule：
a)	每个规则都有dataCode和ruleId两个属性(3.1.1 规则类定义)。在执行规则前，通过规则知道需要解析哪个dataCode(产品编码)下面的设备日志，多个规则可以设置同一个dataCode，即一个dataCode下面的设备日志可以被多个规则处理。
b)	设备日志在被发送到规则引擎前，只有dataCode字段，换而言之，设备日志在发送到规则引擎前，并不知道会被哪一个规则处理。
c)	通过a)和b)的描述，可以知道规则和日志是多对一(n:1)的关系，即一条日志可以被多条规则处理。
d)	通过规则和日志的dataCode字段，日志可以获得规则的ruleId，通过ruleId能够使得日志和规则关联。即：规则引擎知道带上ruleId的日志该由哪一个规则处理。
2.2  流程
前提条件：
1)	kafka的topic(log)中有不断上报的物联设备的日志数据。
2)	规则需要先启动，不需要该规则时候应该停止规则，减少计算资源消耗。
3)	规则更新需要先停止规则，然后再生成新的规则。
如图2-1所示：
1)	业务系统生成规则并发送到Kafka指定的topic(rule)。
2)	Flink从Kafka的topic(logA、logB等)中不间断获取日志(Log)。
3)	Flink从从Kafka的topic(rule)中不间断获取规则(Rule)。
4)	Flink将规则字符串解析成可以使用的规则(ParsedRule)。
5)	在Flink中：
a)	如果是启用规则：
i.	根据规则解析出日志需要匹配的字段，并附上规则ID，转换成新的格式(LogMixRule)。
ii.	生成规则ID对应的SiddhiManager。
iii.	生成符合Siddhi语法的类sql(SQL like language)语言字符串，SiddhiManager根据类sql语言字符串创建SiddhiAppRuntime。
iv.	创建SiddhiAppRuntime的Callback函数用于接收生成的事件，并定义事件的格式(RuleResult)。
v.	获取SiddhiAppRuntime的InputHandle。
vi.	启动SiddhiAppRuntime。
vii.	通过InputHandle将步骤i的日志发送给SiddhiAppRuntime。
viii.	通过Callback获取结果(RuleResult)发送给Flink
ix.	Flink将结果(RuleResult)发送给kafka的topic(ruleResult)。
b)	如果是停止规则：
i.	停止解析对应规则的日志(Log)。
ii.	停止SiddhiManager。
iii.	删除SiddhiManager。
3  规则
3.1  规则定义
3.1.1 规则类定义
规则类定义如表3-1所示。
表3-1 规则类定义
类	说明	字段
RuleInfo	1.	根据规则的json。定义的json的第一层抽象。主要描述了规则的类型，选取的字段，持续时长。
2.	根据规则，解析规则的datacode和其对应的sensorId；规则类型；siddhi所需的类sql字符串。	ruleId:String
enable:Int
groupId:String
dataCode:List[String]
sensorIdsWithDataCode:List[(String,String)]
ruleType:Int
duration:Int
fields:List[String]
fieldsMap:Map[String,String]
count:Int
productInfo: List[ProductInfo]
siddhiSQL:String
ProductInfo	产品信息。	productName:String
dataCode:String
sensorIds: List[String]
rowInfos: List[RowInfo]
RowInfo	Row即是规则条件。	rowName:String
fieldInfos: List[FieldInfo]
rowCondtionTup2:(String,String)
FieldInfo	条件里面的属性、比较符号、属性需要转换的类型、属性比较值。	fieldName:String
convertType:String
compareSymbol:String
compareValue:List[String]
fieldCondTup2:(String,String)
3.1.2 业务JSON定义
{     "ruleId": "eventId_1",     "groupId": "group_1",     "ruleType": "5",     "enable":"0",     "duration": "180",     "count": "4",     "fields":["`fieldA`","`fieldB`","`fieldC`","`fieldD`"],     "productA": {         "dataCode": "dataCodeA",         "sensorIds": [             "dataCodeA_sensorIdA",             "dataCodeA_sensorIdB"         ],         "Row1": {             "`fieldA`": {                 "compare": "eq",                 "type": "int",                 "value": "123"             },             "`fieldB`": {                 "compare": "eq",                 "type": "double",                 "value": "123"             }         },         "Row2": {             "`fieldC`": {                 "compare": "eq",                 "type": "String",                 "value": "123"             },             "`fieldD`": {                 "compare": "eq",                 "type": "String",                 "value": "ux"             }         }     },     "productB": {         "dataCode": "dataCodeB",         "sensorIds": [             "dataCodeB_sensorIdA",             "dataCodeB_sensorIdB"         ],         "Row1": {             "`fieldA`": {                 "compare": "between",                 "type": "string",                 "value": "123,456"             },             "`fieldB`": {                 "compare": "in",                 "type": "string",                 "value": "123,45,6"             }         },         "Row2": {             "`fieldC`": {                 "compare": "notin",                 "type": "String",                 "value": "123,55,77"             },             "`fieldD`": {                 "compare": "between",                 "type": "int",                 "value": "123,45,67"             }         }     } } 各个字段意义如下：
1.	eventId：规则Id，用于不同规则之间的区分。
2.	groupId：在规则判定中没有意义，业务系统使用。
3.	ruleType：规则类型，对应于1.2小节。
4.	duration：cep规则的持续时长，用于规则2，3，4，5。
5.	fields：规则所需要的字段集合。
6.	count：出现次数，用于规则5。
7.	productA：产品信息，可以重复，重复的用productB、productC等。
a)	dataCode：产品编号。
b)	sensorIds：设备编号。
c)	Row1：产品属性条件，可以重复，重复的用Row1、Row2等
i.	`fieldA`：属性名称。
ii.	compare：比较条件。
iii.	type：需要与value比较时的类型。
iv.	value：比较的值。
